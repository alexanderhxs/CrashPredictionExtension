Here are the additional files/changes for the mmcp, to reconstruct the experiments

## Directory Location

All scripts described here are located in the `Crash-Prediction/helpers/` directory.

---

## File Descriptions and Pipeline Integration

### 1. `get_images.py`

*   **Purpose**: This script automates the initial and most crucial pre-processing step: extracting individual image frames from a source video file (e.g., an `.mp4` file). The main pipeline components (`get_depth`, `get_pose`) operate on these static images.
*   **Pipeline Step**: **Step 0 (Data Preparation)**. This script must be run before any other part of the pipeline. It takes a video file as input and produces a folder of `.png` frames in the `../Trajectory Prediction/test_data/` directory.
*   **Difference from Original Repo**: The original repository's documentation assumes that frame extraction has already been done (e.g., from a ROS bag file as mentioned in the main `README.md`). This script provides a standardized way to perform this extraction directly from video files, making the project more accessible.

### 2. `get_ped_gps.py`

*   **Purpose**: This script calculates the relative displacement of pedestrians. It loads the 3D tensor data (generated by `generateTensor_Kalman.py`) and converts the 3D coordinates into real-world (X, Y) displacements relative to the camera's position.
*   **Pipeline Step**: **Step 5 (Relative Displacement Calculation)**. It processes the output from the tensor generation step.
*   **Difference from Original Repo**: In the original repository structure, this script was located at `Trajectory Prediction/get_gps_trajectory/get_ped_gps.py`. Moving it to the `helpers` directory suggests a refactoring to group data processing and conversion utilities together, separating them from the core model-running scripts. The functionality remains the same.

### 3. `format_coordinates.py`

*   **Purpose**: This script takes the pedestrian displacement data generated by `get_ped_gps.py` and formats it into a line-by-line JSON format. Each line represents a single detection (frame ID, pedestrian ID, x-coordinate, y-coordinate), which is a common format for training trajectory prediction models.
*   **Pipeline Step**: **Step 6 (Final Formatting)**. This is the final step in the pedestrian trajectory extraction pipeline, creating the `_atlas.json` files.
*   **Difference from Original Repo**: This script automates the final formatting step. While the original pipeline produced the necessary data, this helper script standardizes the output into a clean, machine-readable format for subsequent use in model training.

### 4. `concat_json.py`

*   **Purpose**: After running the pipeline on multiple video folders and generating several `_atlas.json` files, this script concatenates them into a single, large JSON file. This is useful for creating a unified dataset for training a prediction model.
*   **Pipeline Step**: **Post-processing**. This script is run after the entire pipeline has been executed for all relevant data folders.
*   **Difference from Original Repo**: This is a new utility script that was not present in the original repository. It adds a convenient data management feature for handling multiple datasets.

### 5. `mmcp_atlas_paths.txt`

*   **Purpose**: This text file likely contains the file paths to all the `_atlas.json` files that have been generated. It serves as an input for `concat_json.py`, telling it which files to merge.
*   **Pipeline Step**: **Post-processing**. It is used by `concat_json.py`.
*   **Difference from Original Repo**: This is a new file, introduced to support the functionality of `concat_json.py`.


# Project Setup and Installation

This guide provides instructions on how to set up the necessary environment and dependencies to run the trajectory prediction pipeline.

## 1. Prerequisites

*   **Python**: This project is developed and tested with **Python 3.8**. It is recommended to use this version to avoid potential compatibility issues.
*   **Git**: To clone the repository and fetch model checkpoints.
*   **(Optional) NVIDIA GPU**: For significantly faster processing, an NVIDIA GPU with a compatible CUDA toolkit is recommended.

## 2. Environment Setup

It is highly recommended to use a virtual environment to manage project dependencies.

**1. Create a virtual environment:**
Open a terminal in the project's root directory and run:
```bash
python -m venv .venv
```

**2. Activate the virtual environment:**
*   On **Windows**:
    ```bash
    .\.venv\Scripts\activate
    ```
*   On **macOS/Linux**:
    ```bash
    source .venv/bin/activate
    ```
Your terminal prompt should now be prefixed with `(.venv)`.

## 3. Install Dependencies

### `requirements.txt`

